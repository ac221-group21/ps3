{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/jimwaldo/APCOMP221Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AC 221 - Problem Set 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding Duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know from Problem 2 that there are no incomplete or malformed lines. That means when finding duplicates, we can use pandas and our only concern will be that the two fields we're interested in, `course_id` and `user_id` are read correctly. We can do that by enforcing their data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/justindcclark/miniconda3/envs/school/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3063: DtypeWarning: Columns (47,48,49,53,54,55,56,57,58,59,60,63) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('sample_set.csv', \n",
    "                 parse_dates=['cert_created_date','cert_modified_date', \n",
    "                              'start_time', 'first_event', 'last_event'],\n",
    "                 dtype={\n",
    "                     'course_id': np.str,\n",
    "                     'user_id': np.int64,\n",
    "                     'registered': np.bool, \n",
    "                     'explored': np.bool, \n",
    "                     'un_developing_nation': np.str},\n",
    "                verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1100005, 91)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "962555"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_duplicates = df.duplicated(subset=['course_id', 'user_id'], keep='first').sum()\n",
    "n_duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 962,555 duplicate rows. This count includes all instances of each duplicated row except the first, so it is a count of how many rows would be removed if we were to remove duplicates. We can confirm this by actually dropping the duplicates and counting the rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(137450, 91)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_rows = df.drop_duplicates(subset=['course_id', 'user_id'], keep='first')\n",
    "unique_rows.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_duplicates + unique_rows.shape[0] == df.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our numbers square."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO** check that this squares with [Jim's code](https://github.com/jimwaldo/APCOMP221Code/blob/master/find_duplicates.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Created on 2019-03-14\n",
    "@author waldo\n",
    "\"\"\"\n",
    "\n",
    "import csv\n",
    "\n",
    "def find_dups(csv_in, csv_out):\n",
    "    total_lines = 0\n",
    "    unique_lines = 0\n",
    "    dup_lines = 0\n",
    "    lines_seen = set()\n",
    "\n",
    "    for l in csv_in:\n",
    "        total_lines += 1\n",
    "        key = get_key(l)\n",
    "        if key not in lines_seen:\n",
    "            lines_seen.add(key)\n",
    "            csv_out.writerow(l)\n",
    "            unique_lines += 1\n",
    "        else:\n",
    "            dup_lines += 1\n",
    "    return total_lines, unique_lines, dup_lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Repairing Missing Values\n",
    "\n",
    "**TODO**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2\n",
    "\n",
    "There are numerous ways a line could be corrupt. We will consider lines corrupt that either fail to parse as CSV lines or have fewer fields than the header specifies.\n",
    "First, let's see how many newline characters are in the file. This is an upper bound on the number of rows our CSV should have (because fields can contain newline characters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1100006 sample_set.csv\n"
     ]
    }
   ],
   "source": [
    "!wc -l sample_set.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 1,100,006 lines in the CSV. We can see below that the first line is a header and the second line is where data begins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "course_id,user_id,registered,viewed,explored,certified,completed,ip,cc_by_ip,countryLabel,continent,city,region,subdivision,postalCode,un_major_region,un_economic_group,un_developing_nation,un_special_region,latitude,longitude,LoE,YoB,gender,grade,passing_grade,start_time,first_event,last_event,nevents,ndays_act,nplay_video,nchapters,nforum_posts,nforum_votes,nforum_endorsed,nforum_threads,nforum_comments,nforum_pinned,roles,nprogcheck,nproblem_check,nforum_events,mode,is_active,cert_created_date,cert_modified_date,cert_status,verified_enroll_time,verified_unenroll_time,profile_country,y1_anomalous,email_domain,language_brwsr,language_brwsr_country,language_brwsr_sec,language_brwsr_sec_country,language_brwsr_code,language_brwsr_subcode,language_brwsr_sec_code,language_brwsr_sec_subcode,language_brwsr_nevents,language_brwsr_ndiff,language,language_download,language_nevents,language_ndiff,ntranscript,nshow_answer,nvideo,nvideos_unique_viewed,nvideos_total_watched,nseq_goto,nseek_video,npause_video,avg_dt,sdv_dt,max_dt,n_dt,sum_dt,roles_isBetaTester,roles_isInstructor,roles_isStaff,roles_isCCX,roles_isFinance,roles_isLibrary,roles_isSales,forumRoles_isAdmin,forumRoles_isCommunityTA,forumRoles_isModerator,forumRoles_isStudent\n",
      "HarvardX/PH525.1x/1T2018,4605571,True,True,False,False,False,172.221.204.94,US,United States,North America,,,,,Northern America,Developed regions,,,38.0,-97.0,,,,,0.7,2018-04-29 17:32:35,2018-04-29 17:32:34.982763,2018-05-06 14:14:09.698958,85,15,4,1,,,,,,,Student,0,0,0,audit,1,,,,,,,,hotmail.com,English,United States,,,en,US,,,33,1,,,,,0,0,4,1,0.02702702702702703,4,0,4,25.580624924242418,45.99917905419475,166.573659,66,1243.6008769999999,,,,,,,,,,,1\n"
     ]
    }
   ],
   "source": [
    "!head -n 2 sample_set.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we'd expect at most 1,100,005 rows of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We already have some indication from Problem 1 that there are exactly this number of rows if pandas parses the file for us. Let's do some double checks. For our second pass at counting the number of good rows, we will use an adapted version of [the code](https://github.com/jimwaldo/APCOMP221Code/blob/master/clean_csv.py) Professor Waldo showed in class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv, sys\n",
    "\n",
    "def count_line_status(csv_in):\n",
    "    total_lines = 0\n",
    "    good_lines = 0\n",
    "    bad_lines = 0\n",
    "\n",
    "    header = next(csv_in)\n",
    "    l_len = len(header)\n",
    "    while True:\n",
    "        try:\n",
    "            total_lines += 1\n",
    "            l = next(csv_in)\n",
    "            if len(l) == l_len:\n",
    "                good_lines += 1\n",
    "            else:\n",
    "                bad_lines += 1\n",
    "        except StopIteration:\n",
    "            total_lines -= 1\n",
    "            break\n",
    "        except:\n",
    "            bad_lines += 1\n",
    "            continue\n",
    "    return total_lines, good_lines, bad_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total lines = 1100005 Good lines = 1100005 Bad lines = 0\n"
     ]
    }
   ],
   "source": [
    "with open('sample_set.csv') as f:\n",
    "    reader = csv.reader(f)\n",
    "    total_lines, good_lines, bad_lines = count_line_status(reader)\n",
    "print('Total lines = ' + str(total_lines), 'Good lines = ' + str(good_lines),\n",
    "          'Bad lines = ' + str(bad_lines))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives us further evidence there are zero corrupt lines. Every row parses correctly and every row has exactly 91 fields. Let's do that same thing one more time, but with the CSV parser set to strict so we know it will throw exceptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total lines = 1100005 Good lines = 1100005 Bad lines = 0\n"
     ]
    }
   ],
   "source": [
    "class MyDialect(csv.Dialect):\n",
    "    strict = True\n",
    "    skipinitialspace = True\n",
    "    quoting = csv.QUOTE_ALL\n",
    "    delimiter = ','\n",
    "    quotechar = '\"'\n",
    "    lineterminator = '\\n'\n",
    "    \n",
    "with open('sample_set.csv') as f:\n",
    "    reader = csv.reader(f, MyDialect())\n",
    "    total_lines, good_lines, bad_lines = count_line_status(reader)\n",
    "    \n",
    "print('Total lines = ' + str(total_lines), 'Good lines = ' + str(good_lines),\n",
    "          'Bad lines = ' + str(bad_lines))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're told again there are zero bad lines. As one final check, let's count the number of commas in each line. A well-structured line should have 90 commas separating the 91 fields. If a line was truncated, it will likely have fewer than 90. This is not a strict test because lines may have commas that are not field delimiters but instead part of the field value, but it will likely provide some indication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({90: 1096357, 91: 3624, 92: 25})"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import collections\n",
    "n_commas_per_line = []\n",
    "with open('sample_set.csv') as f:\n",
    "    for line in f:\n",
    "        n_commas = len(line.split(','))-1\n",
    "        n_commas_per_line.append(n_commas)\n",
    "collections.Counter(n_commas_per_line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that every line has at least 90 commas, so between this result and those above, it's quite likely there are no truncated or unparsable lines. As this make answering the rest of the problem less meaningful, I asked on Piazza and was given another dataset which assuredly has corrupt lines against which to test the above methods. Let's see if they work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total lines = 1000000 Good lines = 923993 Bad lines = 76007\n"
     ]
    }
   ],
   "source": [
    "with open('sample_set2.csv') as f:\n",
    "    reader = csv.reader(f)\n",
    "    total_lines, good_lines, bad_lines = count_line_status(reader)\n",
    "print('Total lines = ' + str(total_lines), 'Good lines = ' + str(good_lines),\n",
    "          'Bad lines = ' + str(bad_lines))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bad lines are detected here, so it's safe to assume our method works and there are no bad lines in the original file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO** address rest of question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
