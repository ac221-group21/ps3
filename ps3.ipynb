{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AC 221 - Problem Set 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1\n",
    "\n",
    "### Finding Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "import datetime\n",
    "import ipaddress\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin by reading in the csv with the `csv` Python functionality and using a modified version of [Jim's `find_dups` function](https://github.com/jimwaldo/APCOMP221Code/blob/master/find_duplicates.py) to find duplicates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Useful Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_key(x):\n",
    "    return (x['user_id'], x['course_id'])\n",
    "    \n",
    "# Jim's find_dups Function (Modified to not print out csv with unique values)\n",
    "\"\"\"\n",
    "Created on 2019-03-14\n",
    "Modified on 2020-04-02\n",
    "@author waldo\n",
    "\"\"\"\n",
    "def find_dups(csv_in):\n",
    "    total_lines = 0\n",
    "    unique_lines = 0\n",
    "    dup_lines = 0\n",
    "    lines_seen = set()\n",
    "\n",
    "    for l in csv_in:\n",
    "        total_lines += 1\n",
    "        key = get_key(l)\n",
    "        if key not in lines_seen:\n",
    "            lines_seen.add(key)\n",
    "            unique_lines += 1\n",
    "        else:\n",
    "            dup_lines += 1\n",
    "    return total_lines, unique_lines, dup_lines\n",
    "\n",
    "def get_unique_users_courses(theRecords):\n",
    "    total_records = 0\n",
    "    unique_pairs = 0\n",
    "    pairs_seen = set()\n",
    "\n",
    "    for record in theRecords:\n",
    "        total_records += 1\n",
    "        key = get_key(record)\n",
    "        if key not in pairs_seen:\n",
    "            pairs_seen.add(key)\n",
    "            unique_pairs += 1\n",
    "    return total_records, unique_pairs, pairs_seen\n",
    "\n",
    "def get_user(thePair):\n",
    "    return thePair[0]\n",
    "\n",
    "def get_missing_or_none_array(theRecord):\n",
    "    missing_array = np.zeros((1, len(theRecord.keys())))\n",
    "    for i, key in enumerate(theRecord.keys()):\n",
    "        if theRecord[key] == '' or theRecord[key] == None:\n",
    "            missing_array[0, i] = 1\n",
    "    return missing_array\n",
    "\n",
    "def get_sum_missing_pairs(theRecords, thePair):\n",
    "    sums = []\n",
    "    for record in grouped_records[thePair]:\n",
    "        missings = get_missing_or_none_array(record)\n",
    "        sums.append(np.sum(missings))\n",
    "    return sums\n",
    "\n",
    "# Based on Jim Waldo's find_dups function\n",
    "\"\"\"\n",
    "Created on 2019-03-14\n",
    "@author waldo\n",
    "\"\"\"\n",
    "def create_unique_records_csv(csv_in, csv_out):\n",
    "    total_lines = 0\n",
    "    unique_lines = 0\n",
    "    dup_lines = 0\n",
    "    lines_seen = set()\n",
    "\n",
    "    for l in csv_in:\n",
    "        total_lines += 1\n",
    "        key = get_key(l)\n",
    "        if key not in lines_seen:\n",
    "            lines_seen.add(key)\n",
    "            csv_out.writerow(l)\n",
    "            unique_lines += 1\n",
    "        else:\n",
    "            dup_lines += 1\n",
    "    return total_lines, unique_lines, dup_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open original csv to get headers\n",
    "with open(\"sample_set.csv\", \"r\") as org_csv:\n",
    "    reader = csv.reader(org_csv)\n",
    "    headers = next(reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['course_id', 'user_id', 'registered', 'viewed', 'explored', 'certified', 'completed', 'ip', 'cc_by_ip', 'countryLabel', 'continent', 'city', 'region', 'subdivision', 'postalCode', 'un_major_region', 'un_economic_group', 'un_developing_nation', 'un_special_region', 'latitude', 'longitude', 'LoE', 'YoB', 'gender', 'grade', 'passing_grade', 'start_time', 'first_event', 'last_event', 'nevents', 'ndays_act', 'nplay_video', 'nchapters', 'nforum_posts', 'nforum_votes', 'nforum_endorsed', 'nforum_threads', 'nforum_comments', 'nforum_pinned', 'roles', 'nprogcheck', 'nproblem_check', 'nforum_events', 'mode', 'is_active', 'cert_created_date', 'cert_modified_date', 'cert_status', 'verified_enroll_time', 'verified_unenroll_time', 'profile_country', 'y1_anomalous', 'email_domain', 'language_brwsr', 'language_brwsr_country', 'language_brwsr_sec', 'language_brwsr_sec_country', 'language_brwsr_code', 'language_brwsr_subcode', 'language_brwsr_sec_code', 'language_brwsr_sec_subcode', 'language_brwsr_nevents', 'language_brwsr_ndiff', 'language', 'language_download', 'language_nevents', 'language_ndiff', 'ntranscript', 'nshow_answer', 'nvideo', 'nvideos_unique_viewed', 'nvideos_total_watched', 'nseq_goto', 'nseek_video', 'npause_video', 'avg_dt', 'sdv_dt', 'max_dt', 'n_dt', 'sum_dt', 'roles_isBetaTester', 'roles_isInstructor', 'roles_isStaff', 'roles_isCCX', 'roles_isFinance', 'roles_isLibrary', 'roles_isSales', 'forumRoles_isAdmin', 'forumRoles_isCommunityTA', 'forumRoles_isModerator', 'forumRoles_isStudent']\n"
     ]
    }
   ],
   "source": [
    "print(headers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results for user_id/course_id Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run function\n",
    "with open(\"sample_set.csv\", \"r\", encoding = \"utf8\") as org_file:\n",
    "    org_csv = csv.DictReader(org_file)        \n",
    "    total_lines, unique_lines, dup_lines = find_dups(org_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total lines: 1100005\n",
      "Unique lines: 137450\n",
      "Duplicated lines: 962555\n"
     ]
    }
   ],
   "source": [
    "print(\"Total lines: {}\".format(total_lines))\n",
    "print(\"Unique lines: {}\".format(unique_lines))\n",
    "print(\"Duplicated lines: {}\".format(dup_lines))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Investigating Nature of Duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to know if it will be possible to use duplicates to fill in potential missing or bad values. We will see if these duplicates are exact record duplicates or only at the user_id/course_id level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total records: 1100005\n",
      "Unique Pairs: 137450\n"
     ]
    }
   ],
   "source": [
    "total_records, unique_pairs_n, unique_pairs = get_unique_users_courses(records)\n",
    "print(\"Total records: {}\".format(total_records))\n",
    "print(\"Unique Pairs: {}\".format(unique_pairs_n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_pairs = sorted(unique_pairs, key = get_user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_records = sorted(records, key = get_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "previous_key = ('', '')\n",
    "full_duplicates = []\n",
    "\n",
    "for i, record in enumerate(sorted_records):\n",
    "    current_key = (record['user_id'], record['course_id'])\n",
    "    if current_key == previous_key:\n",
    "        if record == previous_record:\n",
    "            full_duplicates.append(1)\n",
    "        else:\n",
    "            full_duplicates.append(0)\n",
    "            print(current_key, previous_key)\n",
    "    elif previous_key == ('', ''):\n",
    "        previous_key = current_key\n",
    "        previous_record = record  \n",
    "    else:\n",
    "        previous_record = record\n",
    "        previous_key = current_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All duplicates are complete record duplicates.\n"
     ]
    }
   ],
   "source": [
    "if 0 in full_duplicates:\n",
    "    print(\"Not all duplicates are complete record duplicates.\")\n",
    "else:\n",
    "    print(\"All duplicates are complete record duplicates.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like all the duplicates are the same! Thus, we can't use the duplicates to fill in potential missing oe bad values. We will have to repair values some other way.\n",
    "\n",
    "For the rest of the problem set, we will be working with the file of unique records."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating Unique Record File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"sample_set_unique.csv\", \"w\", encoding = \"utf8\") as unique_file:\n",
    "    with open(\"sample_set.csv\", \"r\", encoding = \"utf8\") as org_file:\n",
    "        unique_csv = csv.DictWriter(unique_file, fieldnames = headers)\n",
    "        org_csv = csv.DictReader(org_file)        \n",
    "        total_lines, unique_lines, dup_lines = create_unique_records_csv(org_csv, unique_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total lines: 1100005\n",
      "Unique lines: 137450\n",
      "Duplicated lines: 962555\n"
     ]
    }
   ],
   "source": [
    "print(\"Total lines: {}\".format(total_lines))\n",
    "print(\"Unique lines: {}\".format(unique_lines))\n",
    "print(\"Duplicated lines: {}\".format(dup_lines))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking with Pandas\n",
    "\n",
    "We know from Problem 2 that there are no incomplete or malformed lines. That means when finding duplicates, we can use pandas and our only concern will be that the two fields we're interested in, course_id and user_id are read correctly. We can do that by enforcing their data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Bool column has NA values in column 4",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-97e9735a8a76>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m                      \u001b[1;34m'explored'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbool\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m                      'un_developing_nation': np.str},\n\u001b[1;32m---> 10\u001b[1;33m                 verbose=False)\n\u001b[0m",
      "\u001b[1;32m~\\.conda\\envs\\cs109b\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    674\u001b[0m         )\n\u001b[0;32m    675\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 676\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    678\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\cs109b\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    452\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    453\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 454\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    455\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    456\u001b[0m         \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\cs109b\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1131\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m         \u001b[0mnrows\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_validate_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"nrows\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1133\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1134\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1135\u001b[0m         \u001b[1;31m# May alter columns / col_dict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\cs109b\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   2035\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2036\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2037\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2038\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2039\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_first_chunk\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_low_memory\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_column_data\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_tokens\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_with_dtype\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Bool column has NA values in column 4"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('sample_set.csv', \n",
    "                 parse_dates=['cert_created_date','cert_modified_date', \n",
    "                              'start_time', 'first_event', 'last_event'],\n",
    "                 dtype={\n",
    "                     'course_id': np.str,\n",
    "                     'user_id': np.int64,\n",
    "                     'registered': np.bool, \n",
    "                     'explored': np.bool, \n",
    "                     'un_developing_nation': np.str},\n",
    "                verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_duplicates = df.duplicated(subset=['course_id', 'user_id'], keep='first').sum()\n",
    "n_duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 962,555 duplicate rows. This count includes all instances of each duplicated row except the first, so it is a count of how many rows would be removed if we were to remove duplicates. We can confirm this by actually dropping the duplicates and counting the rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_rows = df.drop_duplicates(subset=['course_id', 'user_id'], keep='first')\n",
    "unique_rows.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_duplicates + unique_rows.shape[0] == df.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get the same results as our more manual approach above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Repairing Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By repairing values we mean to either:\n",
    "* Replace bad values with correct values\n",
    "* Impute missing values\n",
    "\n",
    "First, we want to start by getting a better sense of our data to determine which values need to be repaired. This will also help us determine how to deal with missing values on a field by field basis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read and store unique records\n",
    "records = []\n",
    "\n",
    "with open(\"sample_set_unique.csv\", \"r\", encoding = \"utf8\") as unique_file:\n",
    "    unique_csv = csv.DictReader(unique_file, fieldnames = headers)\n",
    "    for row in unique_csv:\n",
    "        records.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "137450"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(records)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Useful Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions for exploring\n",
    "def column_frequencies(theRecords, theColumn):\n",
    "    frequency_dict = {}\n",
    "    for record in theRecords:\n",
    "        record_column = record[theColumn]\n",
    "        if record_column in frequency_dict:\n",
    "            frequency_dict[record_column] += 1\n",
    "        else:\n",
    "            frequency_dict[record_column] = 1\n",
    "    return frequency_dict\n",
    "\n",
    "def print_sorted_col_by_freq(frequencyDict):\n",
    "    print({k: v for k, v in sorted(frequencyDict.items(), key=lambda item: item[1])})\n",
    "    \n",
    "def print_sorted_col_by_key(frequencyDict, n = 5):\n",
    "    for i, key in enumerate(sorted(frequencyDict.keys())):\n",
    "        if i < n:\n",
    "            print(key, frequencyDict[key])\n",
    "        \n",
    "def print_frequencies_col_group(colGroup, n = 5):\n",
    "    for col in colGroup:\n",
    "        print(col)\n",
    "        freqs = column_frequencies(records, col)\n",
    "        print_sorted_col_by_key(freqs)\n",
    "        print('')\n",
    "        \n",
    "def get_missing_frequency(theRecords, theColumn):\n",
    "    freqs = column_frequencies(theRecords, theColumn)\n",
    "    if '' in freqs.keys():\n",
    "        return freqs['']\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def print_missing_frequencies(theRecords, colGroup):\n",
    "    for col in colGroup:\n",
    "        print(\"{}: {}\".format(col, get_missing_frequency(theRecords, col)))\n",
    "\n",
    "def cast_as_int(theRecord, theCol):\n",
    "    try:\n",
    "        theRecord[theCol] = int(theRecord[theCol])\n",
    "    except ValueError:\n",
    "        theRecord[theCol] = None\n",
    "        \n",
    "def cast_as_float(theRecord, theCol):\n",
    "    try:\n",
    "        theRecord[theCol] = float(theRecord[theCol])\n",
    "    except ValueError:\n",
    "        theRecord[theCol] = None\n",
    "        \n",
    "def remove_none_frequencies(theFrequencies):\n",
    "    if None in theFrequencies.keys():\n",
    "        del theFrequencies[None]\n",
    "    return theFrequencies\n",
    "        \n",
    "def get_min(theRecords, theCol):\n",
    "    freqs = column_frequencies(theRecords, theCol)\n",
    "    freqs_no_none = remove_none_frequencies(freqs)\n",
    "    return min(freqs_no_none.keys())\n",
    "\n",
    "def get_max(theRecords, theCol):\n",
    "    freqs = column_frequencies(theRecords, theCol)\n",
    "    freqs_no_none = remove_none_frequencies(freqs)\n",
    "    return max(freqs_no_none.keys())\n",
    "\n",
    "def get_range(theRecords, theCol):\n",
    "    return (get_min(theRecords, theCol), get_max(theRecords, theCol))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find Bad or Missing Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to determine what values are missing, we need to take a deeper look at the possible values in each column. We will also calculate the frequency of missing (the string '') for each column. We choose to not visualize the output as histograms and instead looked at the string output to make sure we were picking up any bad string values in expected numeric fields. For brevity, we will only output the frequencies of the first five values for each column, but we looked at all values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a look at columns by general categorical group\n",
    "registration_cols = ['registered', 'viewed', 'explored', 'certified', 'completed', 'cert_status']\n",
    "location_cols = ['cc_by_ip', 'countryLabel', 'continent', 'city', 'region', 'subdivision', 'postalCode', \n",
    "                 'profile_country', 'latitude', 'longitude']\n",
    "un_cols = ['un_major_region', 'un_economic_group', 'un_developing_nation', 'un_special_region']\n",
    "demo_cols = ['LoE', 'YoB', 'gender']\n",
    "grade_cols = ['grade', 'passing_grade']\n",
    "time_date_cols = ['start_time', 'first_event', 'last_event', 'cert_modified_date', 'cert_created_date', 'verified_enroll_time',\n",
    "                  'verified_unenroll_time'] \n",
    "action_cols = ['nevents', 'ndays_act'] \n",
    "forum_cols = ['nforum_posts', 'nforum_votes', 'nforum_endorsed', 'nforum_threads', 'nforum_comments', 'nforum_pinned', \n",
    "              'roles', 'nprogcheck', 'nproblem_check', 'nforum_events'] \n",
    "language_cols = ['language_brwsr', 'language_brwsr_country', 'language_brwsr_sec', 'language_brwsr_sec_country', \n",
    "                 'language_brwsr_code', 'language_brwsr_subcode', 'language_brwsr_sec_code', 'language_brwsr_sec_subcode', \n",
    "                 'language_brwsr_nevents', 'language_brwsr_ndiff', \n",
    "                 'language', 'language_download', 'language_nevents', 'language_ndiff']\n",
    "video_cols = ['ntranscript', 'nshow_answer', 'nvideo', 'nplay_video', 'nchapters', 'nvideos_unique_viewed', 'nvideos_total_watched', \n",
    "              'nseq_goto', 'nseek_video', 'npause_video']\n",
    "dt_cols = ['avg_dt', 'sdv_dt', 'max_dt', 'n_dt', 'sum_dt']\n",
    "roles_cols = ['roles_isBetaTester', 'roles_isInstructor', 'roles_isStaff', 'roles_isCCX', 'roles_isFinance', 'roles_isLibrary',\n",
    "              'roles_isSales', 'forumRoles_isAdmin', 'forumRoles_isCommunityTA', 'forumRoles_isModerator', \n",
    "              'forumRoles_isStudent']\n",
    "other_cols = ['mode', 'is_active', 'y1_anomalous', 'email_domain']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Registration Fields*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "registered\n",
      "{'True': 137450}\n",
      "viewed\n",
      "{'True': 71210, 'False': 66240}\n",
      "explored\n",
      "{'False': 56393, 'True': 14817, '': 66240}\n",
      "certified\n",
      "{'False': 135038, 'True': 2412}\n",
      "completed\n",
      "{'False': 134515, 'True': 2935}\n",
      "cert_status\n",
      "{'': 72857, 'downloadable': 2412, 'notpassing': 60837, 'audit_notpassing': 1272, 'audit_passing': 40, 'unverified': 29, 'unavailable': 3}\n"
     ]
    }
   ],
   "source": [
    "for registration_col in registration_cols:\n",
    "    print(registration_col)\n",
    "    print(column_frequencies(records, registration_col))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "registered: 0\n",
      "viewed: 0\n",
      "explored: 531202\n",
      "certified: 0\n",
      "completed: 0\n",
      "cert_status: 582030\n"
     ]
    }
   ],
   "source": [
    "print_missing_frequencies(records, registration_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "81057"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "explored_false_missing = 66240 + 14817\n",
    "explored_false_missing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations:\n",
    "* Are there records where `certified` or `completed` are \"True\" but `explored` is \"False\" or Missing?\n",
    "* We expected `explored` missing or False values might be related to missing values for other variables, particularly those with the same amount of missing such as the forum fields. However, this does not seem to be the case.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Demographic Fields*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoE\n",
      " 19982\n",
      "a 6175\n",
      "b 40552\n",
      "el 608\n",
      "hs 29262\n",
      "\n",
      "YoB\n",
      " 20873\n",
      "1893 1\n",
      "1894 6\n",
      "1895 7\n",
      "1896 4\n",
      "\n",
      "gender\n",
      " 18221\n",
      "f 43041\n",
      "m 75482\n",
      "null 3\n",
      "o 703\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_frequencies_col_group(demo_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations\n",
    "* `LoE` appears to be fine. We decided not to impute as we do not expect values to be missing completely at random. The distributon, at the very least, is probably dependent on the course level. (For example, smeone with a Ph.D probably isn't going to be taking an elementary school course.)\n",
    "* **All `YoB` values, besides missing, are numeric. However, there are quite a few unbelievable values, particularly years in the 1800's, low 1900's, and 2010's. Thus, this variable is a candidate for repair.**\n",
    "* **`gender` has both \"null\" and missing values which we expect to be equivalent in practice. Null might have some significance we don't understand, but considering its small frequency, we are going to treat it as missing. This variable is a candidate for repair.**\n",
    "* We are generally erring on the side of caution when imputing variables. We expect `YoB`, like `LoE` to be correlated with the level of the course among other factors and so we will not impute "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grade\n",
      " 43131\n",
      "0.0 85934\n",
      "0.01 570\n",
      "0.02 354\n",
      "0.03 386\n",
      "\n",
      "passing_grade\n",
      " 3488\n",
      "0.5 8754\n",
      "0.55 129\n",
      "0.58 942\n",
      "0.6 74692\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_frequencies_col_group(grade_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grade: 43131\n",
      "passing_grade: 3488\n"
     ]
    }
   ],
   "source": [
    "print_missing_frequencies(records, grade_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations\n",
    "* The `grade` variable mostly looks reasonable, but there are a few values above 1.00. We suspect some courses offered extra credit. Ideally, we would want to check such courses to make sure this is not an error.\n",
    "* `passing_grade` looks reasonable. Documentation says it should be the same within a course. Ideally, we would check that each course does have the same value for this variable across all students."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Date and Time Fields*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start_time\n",
      "2012-07-24 05:41:08 1\n",
      "2012-07-24 09:27:16 1\n",
      "2012-07-24 10:03:20 1\n",
      "2012-07-24 11:39:48 1\n",
      "2012-07-24 11:52:00 1\n",
      "\n",
      "first_event\n",
      " 31897\n",
      "2012-09-07 01:17:18.096301 1\n",
      "2012-09-07 06:34:28.682564 1\n",
      "2012-09-07 08:03:47.346436 1\n",
      "2012-09-07 09:23:22.404463 1\n",
      "\n",
      "last_event\n",
      " 31897\n",
      "2012-09-07 06:35:12.352788 1\n",
      "2012-09-07 09:49:37.456542 1\n",
      "2012-09-08 16:03:45.943286 1\n",
      "2012-09-08 18:59:09.373412 1\n",
      "\n",
      "cert_modified_date\n",
      " 72857\n",
      "2013-02-01 16:00:58 1\n",
      "2013-02-01 16:01:04 1\n",
      "2013-02-01 16:01:08 1\n",
      "2013-02-01 16:01:11 2\n",
      "\n",
      "cert_created_date\n",
      " 72857\n",
      "2013-02-01 16:00:58 1\n",
      "2013-02-01 16:01:03 1\n",
      "2013-02-01 16:01:08 1\n",
      "2013-02-01 16:01:11 2\n",
      "\n",
      "verified_enroll_time\n",
      " 135738\n",
      "2014-08-07 20:31:17.314671 1\n",
      "2014-09-13 14:59:12.815957 1\n",
      "2014-09-13 21:25:43.286063 1\n",
      "2014-10-08 13:07:14.611714 1\n",
      "\n",
      "verified_unenroll_time\n",
      " 137363\n",
      "2014-10-30 15:54:32.908670 1\n",
      "2014-12-02 22:25:02.047961 1\n",
      "2014-12-30 17:20:32.154019 1\n",
      "2014-12-30 22:46:29.573773 1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_frequencies_col_group(time_date_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "for record in records:\n",
    "    for col in time_date_cols:\n",
    "        if record[col] != '':\n",
    "            try:\n",
    "                datetime.datetime.strptime(record[col], '%Y-%m-%d %H:%M:%S.%f')\n",
    "            except ValueError:\n",
    "                try:\n",
    "                    datetime.datetime.strptime(record[col], '%Y-%m-%d %H:%M:%S')\n",
    "                except ValueError:\n",
    "                    print(record[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start_time: 0\n",
      "first_event: 31897\n",
      "last_event: 31897\n",
      "cert_modified_date: 72857\n",
      "cert_created_date: 72857\n",
      "verified_enroll_time: 135738\n",
      "verified_unenroll_time: 137363\n"
     ]
    }
   ],
   "source": [
    "print_missing_frequencies(records, time_date_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations:\n",
    "* All non-missing date and time fields appear to be in the right format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Action Fields*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nevents\n",
      " 31903\n",
      "1 15588\n",
      "10 1531\n",
      "100 125\n",
      "1000 9\n",
      "\n",
      "ndays_act\n",
      " 31897\n",
      "1 40082\n",
      "10 1405\n",
      "100 10\n",
      "101 10\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_frequencies_col_group(action_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nevents: 31903\n",
      "ndays_act: 31897\n"
     ]
    }
   ],
   "source": [
    "print_missing_frequencies(records, action_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations\n",
    "* These fields all look generally okay. All values are numeric. Registration for the course appears to count as an event and so the counts for these variables start at 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Forum Fields*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nforum_posts\n",
      " 133715\n",
      "1 1508\n",
      "10 54\n",
      "104 1\n",
      "105 1\n",
      "\n",
      "nforum_votes\n",
      " 133715\n",
      "0 2601\n",
      "1 481\n",
      "10 15\n",
      "11 15\n",
      "\n",
      "nforum_endorsed\n",
      " 133715\n",
      "0 3681\n",
      "1 39\n",
      "14 1\n",
      "2 5\n",
      "\n",
      "nforum_threads\n",
      " 133715\n",
      "0 1620\n",
      "1 1113\n",
      "10 25\n",
      "11 20\n",
      "\n",
      "nforum_comments\n",
      " 133715\n",
      "0 1123\n",
      "1 1065\n",
      "10 39\n",
      "100 2\n",
      "\n",
      "nforum_pinned\n",
      " 133715\n",
      "0 3729\n",
      "1 1\n",
      "2 1\n",
      "31 1\n",
      "\n",
      "roles\n",
      "Staff 130\n",
      "Student 137320\n",
      "\n",
      "nprogcheck\n",
      " 31903\n",
      "0 99229\n",
      "1 2259\n",
      "10 87\n",
      "100 1\n",
      "\n",
      "nproblem_check\n",
      " 31903\n",
      "0 93133\n",
      "1 63\n",
      "10 313\n",
      "100 37\n",
      "\n",
      "nforum_events\n",
      " 31903\n",
      "0 98661\n",
      "1 1423\n",
      "10 145\n",
      "100 8\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_frequencies_col_group(forum_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nforum_posts: 133715\n",
      "nforum_votes: 133715\n",
      "nforum_endorsed: 133715\n",
      "nforum_threads: 133715\n",
      "nforum_comments: 133715\n",
      "nforum_pinned: 133715\n",
      "roles: 0\n",
      "nprogcheck: 31903\n",
      "nproblem_check: 31903\n",
      "nforum_events: 31903\n"
     ]
    }
   ],
   "source": [
    "print_missing_frequencies(records, forum_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations\n",
    "* These fields look okay. All values are numeric.\n",
    "* A bit strange that `nforum_posts` starts at 1 when all other forum variables start at 0!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Language Fields*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "language_brwsr\n",
      " 84698\n",
      "Afrikaans 1\n",
      "Albanian 1\n",
      "Arabic 263\n",
      "Bengali 1\n",
      "\n",
      "language_brwsr_country\n",
      " 86205\n",
      "Albania 1\n",
      "Argentina 248\n",
      "Australia 240\n",
      "Austria 17\n",
      "\n",
      "language_brwsr_sec\n",
      " 121488\n",
      "Afrikaans 21\n",
      "Albanian 6\n",
      "Amharic 1\n",
      "Arabic 401\n",
      "\n",
      "language_brwsr_sec_country\n",
      " 129549\n",
      "Afghanistan 1\n",
      "Algeria 1\n",
      "Argentina 8\n",
      "Australia 20\n",
      "\n",
      "language_brwsr_code\n",
      " 84693\n",
      "af 1\n",
      "ar 263\n",
      "bg 44\n",
      "bn 1\n",
      "\n",
      "language_brwsr_subcode\n",
      " 84693\n",
      "419 379\n",
      "AE 18\n",
      "AL 1\n",
      "AR 248\n",
      "\n",
      "language_brwsr_sec_code\n",
      " 119405\n",
      " en 1982\n",
      " fr 55\n",
      " zh 2\n",
      "af 21\n",
      "\n",
      "language_brwsr_sec_subcode\n",
      " 129219\n",
      "419 158\n",
      "AE 6\n",
      "AF 1\n",
      "AR 8\n",
      "\n",
      "language_brwsr_nevents\n",
      " 65425\n",
      "1 4461\n",
      "10 1117\n",
      "100 114\n",
      "1000 5\n",
      "\n",
      "language_brwsr_ndiff\n",
      " 65425\n",
      "1 63726\n",
      "2 6793\n",
      "3 1196\n",
      "4 238\n",
      "\n",
      "language\n",
      " 88835\n",
      "bn 212\n",
      "de 4\n",
      "en 48096\n",
      "es 147\n",
      "\n",
      "language_download\n",
      " 88835\n",
      "0 47084\n",
      "1 937\n",
      "10 13\n",
      "11 6\n",
      "\n",
      "language_nevents\n",
      " 88835\n",
      "1 6734\n",
      "10 945\n",
      "100 67\n",
      "1001 1\n",
      "\n",
      "language_ndiff\n",
      " 88835\n",
      "1 46922\n",
      "10 1\n",
      "2 1194\n",
      "3 316\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_frequencies_col_group(language_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "language_brwsr: 84698\n",
      "language_brwsr_country: 86205\n",
      "language_brwsr_sec: 121488\n",
      "language_brwsr_sec_country: 129549\n",
      "language_brwsr_code: 84693\n",
      "language_brwsr_subcode: 84693\n",
      "language_brwsr_sec_code: 119405\n",
      "language_brwsr_sec_subcode: 129219\n",
      "language_brwsr_nevents: 65425\n",
      "language_brwsr_ndiff: 65425\n",
      "language: 88835\n",
      "language_download: 88835\n",
      "language_nevents: 88835\n",
      "language_ndiff: 88835\n"
     ]
    }
   ],
   "source": [
    "print_missing_frequencies(records, language_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations\n",
    "* The language categorical fields look reasonable. \n",
    "* Some of the integer fields follow the same pattern as other integer fields where some fields start at 1 while others start at 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Video Fields*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ntranscript\n",
      " 31903\n",
      "0 90467\n",
      "1 4477\n",
      "10 226\n",
      "100 9\n",
      "\n",
      "nshow_answer\n",
      " 31903\n",
      "0 98319\n",
      "1 1178\n",
      "10 181\n",
      "100 6\n",
      "\n",
      "nvideo\n",
      " 31903\n",
      "0 60270\n",
      "1 4536\n",
      "10 869\n",
      "100 76\n",
      "\n",
      "nplay_video\n",
      " 31903\n",
      "0 60270\n",
      "1 4536\n",
      "10 869\n",
      "100 76\n",
      "\n",
      "nchapters\n",
      " 66240\n",
      "1 29739\n",
      "10 608\n",
      "11 618\n",
      "12 296\n",
      "\n",
      "nvideos_unique_viewed\n",
      " 91124\n",
      "1 12603\n",
      "10 821\n",
      "100 15\n",
      "101 10\n",
      "\n",
      "nvideos_total_watched\n",
      " 91124\n",
      "0.0022371364653243847 48\n",
      "0.002242152466367713 3187\n",
      "0.0025575447570332483 18\n",
      "0.0026041666666666665 12\n",
      "\n",
      "nseq_goto\n",
      " 31903\n",
      "0 74891\n",
      "1 4133\n",
      "10 689\n",
      "100 26\n",
      "\n",
      "nseek_video\n",
      " 31903\n",
      "0 76032\n",
      "1 2584\n",
      "10 742\n",
      "100 36\n",
      "\n",
      "npause_video\n",
      " 31903\n",
      "0 63626\n",
      "1 5268\n",
      "10 879\n",
      "100 56\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_frequencies_col_group(video_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ntranscript: 31903\n",
      "nshow_answer: 31903\n",
      "nvideo: 31903\n",
      "nplay_video: 31903\n",
      "nchapters: 66240\n",
      "nvideos_unique_viewed: 91124\n",
      "nvideos_total_watched: 91124\n",
      "nseq_goto: 31903\n",
      "nseek_video: 31903\n",
      "npause_video: 31903\n"
     ]
    }
   ],
   "source": [
    "print_missing_frequencies(records, video_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations\n",
    "* Some count variables start at 0 while others start after 0. This is directly related to the missing number differences across the video fields. It seems reasonable to assume that all these fields are being taken from the same pool of data: that is, if a record has a missing value for one of these fields, it has a missing value for all fields. The `nvideos_unique_viewed` and `nvideos_total_watched` have more missing data because they do not include 0. But because `nvideo` is a count of all video events, no matter the kind, we suspect that 60270 of the 91124 of the missing values of those two variables are actually 0. This matches the count of `nplay_video` with a value of 0.\n",
    "* `nvideos_total_watched` is mostly from 0 to 1, but goes up to 6.5. This fields should capture the percentage of all videos watched in a course. We go more into detail in Problem 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further Investigation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "nvideo_flags = []\n",
    "\n",
    "for record in records:\n",
    "    if record['nvideo'] == '0' and record['nvideos_total_watched'] == '' and record['nvideos_unique_viewed'] == '':\n",
    "        nvideo_flags.append(1)\n",
    "    elif record['nvideo'] == '0' and (record['nvideos_total_watched'] != '' and record['nvideos_unique_viewed'] != ''):\n",
    "        nvideo_flags.append(0)\n",
    "    elif record['nvideo'] == '0' and (record['nvideos_total_watched'] != '' or record['nvideos_unique_viewed'] != ''):\n",
    "        nvideo_flags.append(-1)\n",
    "    elif record['nvideo'] == '' and (record['nvideos_total_watched'] == '' or record['nvideos_unique_viewed'] == ''):\n",
    "        nvideo_flags.append(-2)\n",
    "    elif record['nvideo'] == '' and (record['nvideos_total_watched'] != '' or record['nvideos_unique_viewed'] != ''):    \n",
    "        nvideo_flags.append(-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92173\n",
      "59502\n",
      "768\n",
      "0\n",
      "31517\n",
      "386\n"
     ]
    }
   ],
   "source": [
    "print(len(nvideo_flags))\n",
    "for j in [1, 0, -1, -2, -3]:\n",
    "    print(sum(1 for i in nvideo_flags if i == j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.02702702702702703 1\n",
      "0.05405405405405406 2\n",
      "0.05405405405405406 2\n",
      "0.05405405405405406 2\n",
      "0.02702702702702703 1\n",
      "0.02702702702702703 1\n",
      "0.05405405405405406 2\n",
      "0.02702702702702703 1\n",
      "0.023255813953488372 1\n",
      "0.023255813953488372 1\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "\n",
    "for record in records:\n",
    "    if i < 10:\n",
    "        if record['nvideo'] == '0' and (record['nvideos_total_watched'] != '' and record['nvideos_unique_viewed'] != ''):\n",
    "            print(record['nvideos_total_watched'], record['nvideos_unique_viewed'])\n",
    "            i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our suspicion was mostly accurate: 59502 of the 60270 records with 0 `nvideo` had missing values for the other two variables. There were 786 records that had 0 video events from the tracking logs but somehow had recorded a non-zero number of unique videos watched. We suspect that the number of videos watched is calculated separately from the tracking logs, or there is some sort of error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Time Difference Fields*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_dt\n",
      " 54778\n",
      "0.0 5\n",
      "0.000274 1\n",
      "0.000275 1\n",
      "0.000277 1\n",
      "\n",
      "sdv_dt\n",
      " 61344\n",
      "0.0 4\n",
      "0.0007071067811864697 1\n",
      "0.0007071067811865475 1\n",
      "0.0033820917344152437 1\n",
      "\n",
      "max_dt\n",
      " 54778\n",
      "0.0 5\n",
      "0.000274 1\n",
      "0.000275 1\n",
      "0.000277 1\n",
      "\n",
      "n_dt\n",
      " 31897\n",
      "0 22881\n",
      "1 5732\n",
      "10 1321\n",
      "100 145\n",
      "\n",
      "sum_dt\n",
      " 54778\n",
      "0.0 5\n",
      "0.000274 1\n",
      "0.000275 1\n",
      "0.000277 1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_frequencies_col_group(dt_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_dt: 54778\n",
      "sdv_dt: 61344\n",
      "max_dt: 54778\n",
      "n_dt: 31897\n",
      "sum_dt: 54778\n"
     ]
    }
   ],
   "source": [
    "print_missing_frequencies(records, dt_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obserations\n",
    "* None of the values look unreasonable and all values are numeric, but we are generally unaware what the bounds should be. This might vary by course. (For example, a course that is only open for $x$ total seconds should not have a `max_dt` value exceeding $x$.) When looking at ranges below, we will be able to see if the largest values seem to be at a reasonable scale.\n",
    "* The difference in missing values across these five variables is unexpected as we expected these variables to be collected from the same source. (For example, why would a standard deviation of time difference exist but not the number of time difference events which are necessary for the computations?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Role Fields*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roles_isBetaTester\n",
      " 137439\n",
      "1 11\n",
      "\n",
      "roles_isInstructor\n",
      " 137386\n",
      "1 64\n",
      "\n",
      "roles_isStaff\n",
      " 137392\n",
      "1 58\n",
      "\n",
      "roles_isCCX\n",
      " 137450\n",
      "\n",
      "roles_isFinance\n",
      " 137449\n",
      "1 1\n",
      "\n",
      "roles_isLibrary\n",
      " 137450\n",
      "\n",
      "roles_isSales\n",
      " 137449\n",
      "1 1\n",
      "\n",
      "forumRoles_isAdmin\n",
      " 137434\n",
      "1 16\n",
      "\n",
      "forumRoles_isCommunityTA\n",
      " 137450\n",
      "\n",
      "forumRoles_isModerator\n",
      " 137436\n",
      "1 14\n",
      "\n",
      "forumRoles_isStudent\n",
      " 6\n",
      "1 137444\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_frequencies_col_group(roles_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations\n",
    "* All fields are either 1 or missing. This suggests there is no \"true\" missing value as 0 (definitely not part of the role) and missing appear to have been collapsed together.\n",
    "* No one in this sample has role of CCX, Library, or CommunityTA. If we knew more about the sample, this might be a potential redflag. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*IP Address*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "for record in records:\n",
    "    if record['ip'] != '':\n",
    "        try:\n",
    "            ipaddress.ip_address(record['ip'])\n",
    "        except ValueError:\n",
    "            print(record['ip'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20698"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_missing_frequency(records, 'ip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IP Address Questions\n",
    "* All non-missing IP Addresses had a correct format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Location Fields*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cc_by_ip\n",
      " 23167\n",
      "AD 3\n",
      "AE 430\n",
      "AF 27\n",
      "AG 9\n",
      "\n",
      "countryLabel\n",
      " 23167\n",
      "Afghanistan 27\n",
      "Albania 113\n",
      "Algeria 201\n",
      "American Samoa 1\n",
      "\n",
      "continent\n",
      " 22970\n",
      "Africa 6219\n",
      "Americas 48453\n",
      "Asia 28019\n",
      "Europe 24161\n",
      "\n",
      "city\n",
      " 38883\n",
      "'s-gravendeel 1\n",
      "'s-hertogenbosch 4\n",
      "'t Horntje 1\n",
      "A Coruña 17\n",
      "\n",
      "region\n",
      " 43383\n",
      "00 403\n",
      "01 634\n",
      "02 242\n",
      "025 1\n",
      "\n",
      "subdivision\n",
      " 41248\n",
      "A Coruña 40\n",
      "Aargau 13\n",
      "Aberdeen City 18\n",
      "Aberdeenshire 1\n",
      "\n",
      "postalCode\n",
      " 84682\n",
      "00011 1\n",
      "00012 1\n",
      "0002 1\n",
      "00027 1\n",
      "\n",
      "profile_country\n",
      " 52674\n",
      "AD 7\n",
      "AE 286\n",
      "AF 79\n",
      "AG 7\n",
      "\n",
      "latitude\n",
      " 22970\n",
      "-0.0333 1\n",
      "-0.1 1\n",
      "-0.2167 174\n",
      "-0.25 1\n",
      "\n",
      "longitude\n",
      " 22970\n",
      "-0.0001 6\n",
      "-0.0013 5\n",
      "-0.0031 4\n",
      "-0.0104 1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_frequencies_col_group(location_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cc_by_ip: 23167\n",
      "countryLabel: 23167\n",
      "continent: 22970\n",
      "city: 38883\n",
      "region: 43383\n",
      "subdivision: 41248\n",
      "postalCode: 84682\n",
      "profile_country: 52674\n",
      "latitude: 22970\n",
      "longitude: 22970\n"
     ]
    }
   ],
   "source": [
    "print_missing_frequencies(records, location_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations\n",
    "* The first three city names that begin with an apostrophe might have been an encoding error, but no, those are the real names of the cities in the Netherlands.\n",
    "* Below we will check that latitude and longitude are in the proper range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "un_major_region\n",
      " 23167\n",
      "Australia and New Zealand 2698\n",
      "Caribbean 724\n",
      "Central America 2679\n",
      "Central Asia 220\n",
      "\n",
      "un_economic_group\n",
      " 23167\n",
      "Developed regions 69462\n",
      "Developing_Nations 44821\n",
      "\n",
      "un_developing_nation\n",
      " 135362\n",
      "Least developed countries 2088\n",
      "\n",
      "un_special_region\n",
      " 121866\n",
      "Latin America and the Caribbean 11460\n",
      "Sub-Saharan-Africa 4124\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_frequencies_col_group(un_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "un_major_region: 23167\n",
      "un_economic_group: 23167\n",
      "un_developing_nation: 135362\n",
      "un_special_region: 121866\n"
     ]
    }
   ],
   "source": [
    "print_missing_frequencies(records, un_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations:\n",
    "* As with the roles fields, there is no distinction between legitimate missing and not part of any of the categories. We could use `un_major_region` to get a \"real\" missing category and \"Not a Developing Nation\" or \"Not a Special Region\" categories. \n",
    "* **n_developing_nation` and `un_special_region` are candidates for repair.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Misc Fields*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mode\n",
      " 917\n",
      "audit 60266\n",
      "honor 74474\n",
      "verified 1793\n",
      "\n",
      "is_active\n",
      " 917\n",
      "0 20482\n",
      "1 116051\n",
      "\n",
      "y1_anomalous\n",
      " 136531\n",
      "1 919\n",
      "\n",
      "email_domain\n",
      "013.net 1\n",
      "10count.ca 1\n",
      "126.COM 1\n",
      "126.com 232\n",
      "139.com 9\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_frequencies_col_group(other_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mode: 917\n",
      "is_active: 917\n",
      "y1_anomalous: 136531\n",
      "email_domain: 0\n"
     ]
    }
   ],
   "source": [
    "print_missing_frequencies(records, other_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations\n",
    "* We expected `y1_anomalous` might be tied to some missing data since the user \"had to be side loaded from mongo\" but this variable seems to only be obviously related to `mode` and `is_active`. \n",
    "* **Email domain could use some cleaning - forcing everything to lower case. This is a candidate for repair.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Numeric Fields*\n",
    "\n",
    "We did not find any bad string values in expected numeric fields. Thus, we can cast those fields as float or integer for easier analysis of the fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cast_as_int(theRecord, theCol):\n",
    "    try:\n",
    "        theRecord[theCol] = int(theRecord[theCol])\n",
    "    except ValueError:\n",
    "        theRecord[theCol] = None\n",
    "        \n",
    "def cast_as_float(theRecord, theCol):\n",
    "    try:\n",
    "        theRecord[theCol] = float(theRecord[theCol])\n",
    "    except ValueError:\n",
    "        theRecord[theCol] = None\n",
    "        \n",
    "def remove_none_frequencies(theFrequencies):\n",
    "    if None in theFrequencies.keys():\n",
    "        del theFrequencies[None]\n",
    "    return theFrequencies\n",
    "        \n",
    "def get_min(theRecords, theCol):\n",
    "    freqs = column_frequencies(theRecords, theCol)\n",
    "    freqs_no_none = remove_none_frequencies(freqs)\n",
    "    return min(freqs_no_none.keys())\n",
    "\n",
    "def get_max(theRecords, theCol):\n",
    "    freqs = column_frequencies(theRecords, theCol)\n",
    "    freqs_no_none = remove_none_frequencies(freqs)\n",
    "    return max(freqs_no_none.keys())\n",
    "\n",
    "def get_range(theRecords, theCol):\n",
    "    return (get_min(theRecords, theCol), get_max(theRecords, theCol))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cast appropriate fields as numeric\n",
    "records_with_numeric = deepcopy(records)\n",
    "\n",
    "for record in records_with_numeric:\n",
    "    cast_as_float(record, 'latitude')\n",
    "    cast_as_float(record, 'longitude')\n",
    "    cast_as_int(record, 'YoB')\n",
    "    cast_as_float(record, 'grade')\n",
    "    cast_as_float(record, 'passing_grade')\n",
    "    cast_as_int(record, 'nevents')\n",
    "    cast_as_int(record, 'ndays_act')\n",
    "    cast_as_int(record, 'nplay_video')\n",
    "    cast_as_int(record, 'nchapters')\n",
    "    cast_as_int(record, 'nforum_posts')\n",
    "    cast_as_int(record, 'nforum_votes')\n",
    "    cast_as_int(record, 'nforum_endorsed')\n",
    "    cast_as_int(record, 'nforum_threads')\n",
    "    cast_as_int(record, 'nforum_comments')\n",
    "    cast_as_int(record, 'nforum_pinned')\n",
    "    cast_as_int(record, 'nprogcheck')\n",
    "    cast_as_int(record, 'nproblem_check')\n",
    "    cast_as_int(record, 'nforum_events')\n",
    "    cast_as_int(record, 'language_brwsr_nevents')\n",
    "    cast_as_int(record, 'language_brwsr_ndiff')\n",
    "    cast_as_int(record, 'language_download')\n",
    "    cast_as_int(record, 'language_nevents')\n",
    "    cast_as_int(record, 'language_ndiff')\n",
    "    cast_as_int(record, 'ntranscript')\n",
    "    cast_as_int(record, 'nshow_answer')\n",
    "    cast_as_int(record, 'nvideo')\n",
    "    cast_as_int(record, 'nvideos_unique_viewed')\n",
    "    cast_as_float(record, 'nvideos_total_watched')\n",
    "    cast_as_int(record, 'nseq_goto')\n",
    "    cast_as_int(record, 'nseek_video')\n",
    "    cast_as_int(record, 'npause_video')\n",
    "    cast_as_float(record, 'avg_dt')\n",
    "    cast_as_float(record, 'sdv_dt')\n",
    "    cast_as_float(record, 'max_dt')\n",
    "    cast_as_float(record, 'n_dt')\n",
    "    cast_as_float(record, 'sum_dt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at Ranges of Numeric Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "latitude: (-54.8, 71.285)\n",
      "longitude: (-175.2018, 178.4167)\n",
      "YoB: (1893, 2018)\n",
      "grade: (0.0, 1.15)\n",
      "passing_grade: (0.5, 0.97)\n",
      "nevents: (1, 234385)\n",
      "ndays_act: (1, 2475)\n",
      "nplay_video: (0, 90057)\n",
      "nchapters: (1, 34)\n",
      "nforum_posts: (1, 299)\n",
      "nforum_votes: (0, 224)\n",
      "nforum_endorsed: (0, 30)\n",
      "nforum_threads: (0, 142)\n",
      "nforum_comments: (0, 281)\n",
      "nforum_pinned: (0, 31)\n",
      "nprogcheck: (0, 643)\n",
      "nproblem_check: (0, 26367)\n",
      "nforum_events: (0, 17829)\n",
      "language_brwsr_nevents: (1, 114745)\n",
      "language_brwsr_ndiff: (1, 8)\n",
      "language_nevents: (1, 6051)\n",
      "language_ndiff: (1, 10)\n",
      "ntranscript: (0, 4346)\n",
      "nshow_answer: (0, 3432)\n",
      "nvideo: (0, 90057)\n",
      "nvideos_unique_viewed: (1, 363)\n",
      "nvideos_total_watched: (0.0022371364653243847, 6.5)\n",
      "nseq_goto: (0, 6941)\n",
      "nseek_video: (0, 10398)\n",
      "npause_video: (0, 57069)\n",
      "avg_dt: (0.0, 299.589759)\n",
      "sdv_dt: (0.0, 209.66384207378675)\n",
      "max_dt: (0.0, 300.0)\n",
      "n_dt: (0.0, 233917.0)\n",
      "sum_dt: (0.0, 2895426.074810009)\n"
     ]
    }
   ],
   "source": [
    "quant_cols = ['latitude', 'longitude', 'YoB', 'grade', 'passing_grade', \n",
    "              'nevents', 'ndays_act', 'nplay_video', 'nchapters', 'nforum_posts', 'nforum_votes', 'nforum_endorsed', \n",
    "              'nforum_threads', 'nforum_comments', 'nforum_pinned', 'nprogcheck', 'nproblem_check', 'nforum_events', \n",
    "              'language_brwsr_nevents', 'language_brwsr_ndiff', 'language_nevents', 'language_ndiff', 'ntranscript', \n",
    "              'nshow_answer', 'nvideo', 'nvideos_unique_viewed', 'nvideos_total_watched', 'nseq_goto', 'nseek_video', \n",
    "              'npause_video', 'avg_dt', 'sdv_dt', 'max_dt', 'n_dt', 'sum_dt']\n",
    "\n",
    "for col in quant_cols:\n",
    "    print(\"{}: {}\".format(col, get_range(records_with_numeric, col)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations\n",
    "* Latitude and longitude are in the right ranges.\n",
    "* As we mentioned above, some count variables start at 1 while others at 0. \n",
    "    * For `nevents` and `ndays_act`, a minimum of 1 is reasonable if registering for the course counts as an activity. `nchapters` might be similar if the user automatically sees the first chapter upon registering. \n",
    "    * For the language browser fields, a minimum value of 1 is also reasonable if all studenst that do not engage with the video transcript receive a missing value.\n",
    "    * For other variables that have both, what's the difference between 0 and missing? We decided that some more digging would have to be done before repairing such values. Our final dataset would include documentation explaining that some count variables do not include 0. \n",
    "* The total elapsed time spent on a course `sum_dt` was around 33 days, which seems reasonable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Repairing Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will repair the dataset with duplicates removed and with fields still as strings. We erred on the side of caution in repairing values because we do not know what the use of the dataset would be. We only repaired values for which we expected little or no bias introduced or were confident that we knew what the true value should be. For this reason, we would also leave imputation up to the individual analysts and would instead focus on providing strong documentation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create deepcopy of records\n",
    "cleaned_records = deepcopy(records)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*YoB*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This variable included a few unbelievable values, particularly years in the 1800's, low 1900's, and 2010's. We will use  somewhat arbitrary (and slightly ageist) cutoffs. We collapsed unreasonable values into string categories. Our cutoffs were:\n",
    "* Any value below 1920 (which would put the student at 92-years-old if the course was at the time of edX's launch)\n",
    "* Any value above 2007 (which would put the student at about 13 years-old if the course was from this year)\n",
    "\n",
    "A better, but much more involved, method would be to consider the year in which the course ran and try to correct for values where a student incorrectly put their birth year as the current year. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "for record in cleaned_records:\n",
    "    if record['YoB'] != '':\n",
    "        if int(record['YoB']) < 1920:\n",
    "            record['YoB'] = \"< 1920\"\n",
    "        elif int(record['YoB']) > 2007:\n",
    "            record['YoB'] = \"> 2007\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Gender*\n",
    "\n",
    "This field had 3 'null' values that we will collapse as missing. Null might have some significance that we don't understand, but we feel confident collapsing the value into missing because of its small frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "for record in cleaned_records:\n",
    "    if record['gender'] == 'null':\n",
    "        record['gender'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'': 18224, 'm': 75482, 'f': 43041, 'o': 703}"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "column_frequencies(cleaned_records, 'gender')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Email Domain*\n",
    "\n",
    "We want  the email domain variable to hold completely lower-case strings for proper grouping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "for record in cleaned_records:\n",
    "    record['email_domain'] = record['email_domain'].lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*UN Fields*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We assumed that if the `un_major_region` field was not missing, it was possible to determine if the region was a developing nation or special region. Thus, we created a category to capture regions that were not really missing but did not fit in the current categories. This resulted in an equal missing rate across all four variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "for record in cleaned_records:\n",
    "    if record['un_developing_nation'] == '':\n",
    "        if record['un_major_region'] != '':\n",
    "            record['un_developing_nation'] = \"Not a least developed country\"\n",
    "    if record['un_special_region'] == '':\n",
    "        if record['un_major_region'] != '':\n",
    "            record['un_special_region'] = \"Not a UN special region\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "un_major_region: 23167\n",
      "un_economic_group: 23167\n",
      "un_developing_nation: 23167\n",
      "un_special_region: 23167\n"
     ]
    }
   ],
   "source": [
    "print_missing_frequencies(cleaned_records, un_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Video Fields*\n",
    "\n",
    "We will repair the 59502 records with a value of 0 in `nvideo` but missing values for `nvideos_total_watched` and `nvideos_unique_viewed`. This leaves 1154 records with either 0 or missing `nvideo` values but nonzero, nonmissing values for the other two variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "for record in cleaned_records:\n",
    "    if record['nvideo'] == '0' and record['nvideos_total_watched'] == '' and record['nvideos_unique_viewed'] == '':\n",
    "        record['nvideos_total_watched'] = 0\n",
    "        record['nvideos_unique_viewed'] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaned Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"sample_set_unique_cleaned.csv\", \"w\", encoding = \"utf8\") as cleaned_file:\n",
    "    cleaned_csv = csv.DictWriter(cleaned_file, fieldnames = headers)\n",
    "    cleaned_csv.writeheader()\n",
    "    cleaned_csv.writerows(cleaned_records)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are numerous ways a line could be corrupt. We will consider lines corrupt that either fail to parse as CSV lines or have fewer fields than the header specifies. First, let's see how many newline characters are in the file. This is an upper bound on the number of rows our CSV should have (because fields can contain newline characters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'wc' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!wc -l sample_set.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 1,100,006 lines in the CSV. We can see below that the first line is a header and the second line is where data begins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -n 2 sample_set.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "So we'd expect at most 1,100,005 rows of data.\n",
    "\n",
    "We already have some indication from Problem 1 that there are exactly this number of rows if pandas parses the file for us. Let's do some double checks. For our second pass at counting the number of good rows, we will use an adapted version of the code Professor Waldo showed in class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv, sys\n",
    "\n",
    "def count_line_status(csv_in):\n",
    "    total_lines = 0\n",
    "    good_lines = 0\n",
    "    bad_lines = 0\n",
    "\n",
    "    header = next(csv_in)\n",
    "    l_len = len(header)\n",
    "    while True:\n",
    "        try:\n",
    "            total_lines += 1\n",
    "            l = next(csv_in)\n",
    "            if len(l) == l_len:\n",
    "                good_lines += 1\n",
    "            else:\n",
    "                bad_lines += 1\n",
    "        except StopIteration:\n",
    "            total_lines -= 1\n",
    "            break\n",
    "        except:\n",
    "            bad_lines += 1\n",
    "            continue\n",
    "    return total_lines, good_lines, bad_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('sample_set.csv') as f:\n",
    "    reader = csv.reader(f)\n",
    "    total_lines, good_lines, bad_lines = count_line_status(reader)\n",
    "print('Total lines = ' + str(total_lines), 'Good lines = ' + str(good_lines),\n",
    "          'Bad lines = ' + str(bad_lines))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives us further evidence there are zero corrupt lines. Every row parses correctly and every row has exactly 91 fields. Let's do that same thing one more time, but with the CSV parser set to strict so we know it will throw exceptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDialect(csv.Dialect):\n",
    "    strict = True\n",
    "    skipinitialspace = True\n",
    "    quoting = csv.QUOTE_ALL\n",
    "    delimiter = ','\n",
    "    quotechar = '\"'\n",
    "    lineterminator = '\\n'\n",
    "    \n",
    "with open('sample_set.csv') as f:\n",
    "    reader = csv.reader(f, MyDialect())\n",
    "    total_lines, good_lines, bad_lines = count_line_status(reader)\n",
    "    \n",
    "print('Total lines = ' + str(total_lines), 'Good lines = ' + str(good_lines),\n",
    "          'Bad lines = ' + str(bad_lines))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're told again there are zero bad lines. As one final check, let's count the number of commas in each line. A well-structured line should have 90 commas separating the 91 fields. If a line was truncated, it will likely have fewer than 90. This is not a strict test because lines may have commas that are not field delimiters but instead part of the field value, but it will likely provide some indication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "n_commas_per_line = []\n",
    "with open('sample_set.csv') as f:\n",
    "    for line in f:\n",
    "        n_commas = len(line.split(','))-1\n",
    "        n_commas_per_line.append(n_commas)\n",
    "collections.Counter(n_commas_per_line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that every line has at least 90 commas, so between this result and those above, it's quite likely there are no truncated or unparsable lines. As this make answering the rest of the problem less meaningful, I asked on Piazza and was given another dataset which assuredly has corrupt lines against which to test the above methods. Let's see if they work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('sample_set2.csv') as f:\n",
    "    reader = csv.reader(f)\n",
    "    total_lines, good_lines, bad_lines = count_line_status(reader)\n",
    "print('Total lines = ' + str(total_lines), 'Good lines = ' + str(good_lines),\n",
    "          'Bad lines = ' + str(bad_lines))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bad lines are detected here, so it's safe to assume our method works and there are no bad lines in the original file.\n",
    "\n",
    "How many corrupt lines are there? Zero.\n",
    "\n",
    "Does the count of corrupt lines change if you get rid of them before getting rid of the duplicate records? No. It's still zero.\n",
    "\n",
    "What difference might this make to the remaining data set? None. There are zero corrupt lines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sources of Bias\n",
    "* We don't really know how the dataset was collected, so there could potentially be bias concerns in collection.\n",
    "* We also don't know how this dataset was sampled. The name of the file suggests there was some sampling that happened somewhere during the construction of this dataset, and it's possible bias was introduced by the sampling procedure.\n",
    "* We don't know why there are duplicate lines.\n",
    "\n",
    "### Documentation of Unusual Records or Values\n",
    "\n",
    "#### General\n",
    "* There are a large number of duplicate lines.\n",
    "* There are no corrupt lines. We had some expectation that there would be, and given that there aren't, we may want to investigate why there aren't corrupt lines.\n",
    "\n",
    "#### Year of Birth\n",
    "* The original file included some unusual values for year of birth in the 1800's, low 1900's, and 2010's. We created categorical flags for these values. We suggest that analysts using our cleaned dataset create a separate numeric column that sets such values as missing.\n",
    "\n",
    "#### Count Variables\n",
    "We urge analysts using this dataset pay special attention to count variables as they sometimes do not include 0.\n",
    "* `nevents` and `ndays_act` do not include 0 because registration counts as an event.\n",
    "* We are not sure why `nforum_posts` does not include 0.\n",
    "* We are not sure why `nchapters` does not include 0.\n",
    "* For the language browser fields, a minimum value of 1 is also reasonable if all students that do not engage with the video transcript receive a missing value.\n",
    "* nvideos_total_watched is mostly from 0 to 1, but goes up to 6.5. This fields should capture the percentage of all videos watched in a course. Possible reasons for this anomaly are:\n",
    "    * We suspect if the value is greater than 1, a student is watching videos multiple times. (This might mean that a student has not actually watched all videos, but watched some multiple times and others not at all. This would have quite an effect on analysis so we should try to figure out how this field is actually calculated.\n",
    "    * If the value is greater than 1, it might be an issue in the calculation not scaling to percent. (For example, 6.5 should be 0.65 or 0.065.) There is no way of knowing this unless we know how this field is actually calculated.\n",
    "    \n",
    "#### Time Difference\n",
    "* The difference in missing values across the five time difference variables is unusual as we expected these variables to be collected from the same source. (For example, why would a standard deviation of time difference exist but not the number of time difference events which are necessary for the computation of standard deviation?)\n",
    "\n",
    "#### Unusual Binary Fields\n",
    "* The role and forum role variables do not have values of 0, only 1 or missing. This means an analyst might not be able to differentiate between students who were truly not considered as that particular role versus for whom this data might be missing.\n",
    "* Additionally, no one in the sample had a role of CCX, Library, or Community TA. If we knew more about the sample, this might be a potential redflag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
